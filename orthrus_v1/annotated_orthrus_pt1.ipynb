{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yc386/orthrus_metaproteomics/blob/main/annotated_orthrus_pt1_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpisBTGyhi5v"
   },
   "source": [
    "<img src='https://drive.google.com/uc?export=view&id=19rmmQI1H2nIqgU598WROTcUNhOUoXcBP' width='400px' align='right'>\n",
    "\n",
    "# **Readme**\n",
    "\n",
    "---\n",
    "Orthrus 🐾 is a hybrid, two-software pipeline that integrates _de novo_ peptide sequencing (you have the choice between the [InstaNovo](https://github.com/instadeepai/instanovo) and [Casanovo](https://github.com/Noble-Lab/casanovo) implementations) with [Sage](https://github.com/lazear/sage) (a fast database search engine with advanced features like retention time alignment and machine learning-based rescoring).\n",
    "\n",
    "Designed to handle large search space and difficulties of selecting databases in metaproteomics and palaeoproteomics, Orthrus leverages de novo sequencing to define sample-specific databases, and uses probability ranking and conventional database searching to control FDRs (false discovery rates).\n",
    "\n",
    "Orthrus can be run online using Google Colab 🥳, or locally via Anaconda 🐍.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fva_FXk0htYl"
   },
   "source": [
    "# **Please note**❗️\n",
    "*   Before walking the dog, please change the runtime type to GPU (A100, L4, or T4. A100 most efficient but T4 is free)\n",
    "*   If you would like to connect your Google Drive, click the folder image 🗂️ on the left and mount the drive.\n",
    "*  Click `File` (top left) and save a copy in Drive or Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-K0wYqUruZmx"
   },
   "source": [
    "# Run `Orthrus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def run_command(command: str):\n",
    "    \"\"\"Runs a shell command and streams its output in real-time.\"\"\"\n",
    "    process = subprocess.Popen(\n",
    "        command,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        shell=True,  # Enables the use of a string command\n",
    "        text=True,  # Ensures output is in string format\n",
    "    )\n",
    "\n",
    "    # Stream the output\n",
    "    while True:\n",
    "        output = process.stdout.readline()  # type: ignore\n",
    "        if output == \"\" and process.poll() is not None:\n",
    "            break\n",
    "        if output:\n",
    "            print(output.strip())\n",
    "\n",
    "    # Check for errors\n",
    "    stderr = process.stderr.read()  # type: ignore\n",
    "    if stderr:\n",
    "        print(f\"Error: {stderr.strip()}\")\n",
    "\n",
    "    # Return the exit code\n",
    "    return process.returncode\n",
    "\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "    print(\"Running in Colab\")\n",
    "    run_command(\"pip install -q condacolab\")\n",
    "    import condacolab\n",
    "\n",
    "    condacolab.install()\n",
    "    print(\n",
    "        \"Your session has restarted. You can ignore the 'Your session crashed for an unknown reason' warning.\"\n",
    "    )\n",
    "else:\n",
    "    print(\"NOT running in Colab\")\n",
    "    print(\"Adding 'orthrus' conda environment to notebook\")\n",
    "    run_command(\"python -m ipykernel install --user --name=orthrus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running on Colab, please restart your runtime and run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "    # TODO: update this  to the correct repository and branch once merged\n",
    "    url = \"https://raw.githubusercontent.com/BioGeek/orthrus_metaproteomics/refs/heads/aichor/environment.yml\"\n",
    "    urllib.request.urlretrieve(url, \"environment.yml\")\n",
    "    run_command(\"conda env update -n base -f environment.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Cv26MhBMhbT5"
   },
   "outputs": [],
   "source": [
    "#@title Add inputs -> click `Runtime` -> `Run all`\n",
    "#@markdown **_De novo_ peptide sequencing algorithm inputs**\n",
    "algorithm = \"instanovo\"  #@param [\"instanovo\", \"casanovo\"]\n",
    "#@markdown - use the drop-down menu to choose the de novo sequencing algorithm\n",
    "\n",
    "folder_path = \"./data/PXD027613/mzML\"  #@param {type:\"string\"}\n",
    "#@markdown - a folder contains single or multiple `.mzML` or `.mgf` files for the de novo sequencing algorithm (`Instanovo` or `Casanovo`). Please check only _ (underscore) and no other special characters or space in a file name.\n",
    "file_type = \"mzML\"  #@param [\"mzML\", \"mgf\"]\n",
    "#@markdown - use the drop-down menu to choose the instrument file type\n",
    "\n",
    "use_default = True  #@param {type:\"boolean\"}\n",
    "#@markdown **Advanced Options (ignored if using default settings)**\n",
    "\n",
    "checkpoint = \"path/to/model.ckpt\"  #@param {type:\"string\"}\n",
    "#@markdown - path to a checkpoint `.ckpt` for a de novo peptide sequencing model\n",
    "config = \"path/to/config.yaml\"  #@param {type:\"string\"}\n",
    "#@markdown - a `.yaml` configuration file for Casanovo\n",
    "\n",
    "#@markdown **Inputs for converting Casanovo results to a `.fasta`**\n",
    "use_SwissProt = True  #@param {type:\"boolean\"}\n",
    "#@markdown - use the latest, reviewed SwissProt form the UniProt FTP\n",
    "database_path = \"\"  #@param {type:\"string\"}\n",
    "#@markdown - path to a database (`.fasta`) for shortlisting proteins based on de novo results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9pGGlDUimMo",
    "outputId": "a8e81df3-8963-4180-d01e-6e99b37cbf75"
   },
   "outputs": [],
   "source": [
    "#@title install dependencies & modules\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "from pyteomics import mztab\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "pattern = re.compile(r\"(.\\d*\\.?\\d+)\")\n",
    "\n",
    "\n",
    "def prep_mztab(mztab_path):\n",
    "    \"\"\"Parse a .mztab file using pyteomics.\n",
    "\n",
    "    add naked sequences (without PTMs) & the sequence length\n",
    "    input=path to .mztab file\n",
    "    output=pandas dataframe\n",
    "    \"\"\"\n",
    "    m = mztab.MzTab(mztab_path)\n",
    "    df = m.spectrum_match_table\n",
    "    if df is None or df.empty:\n",
    "        raise ValueError(f\"{mztab_path} is empty\")\n",
    "    if \"sequence\" not in df.columns:\n",
    "        raise KeyError(f\"'sequence' column is missing in the file: {mztab_path}\")\n",
    "    df.reset_index(drop=True)\n",
    "    df1 = df.assign(sequence_naked=df[\"sequence\"].str.replace(pattern, \"\", regex=True))\n",
    "    df2 = df1.assign(nAA=df1[\"sequence_naked\"].str.len())\n",
    "    df3 = (\n",
    "        df2.sort_values(by=\"sequence_naked\")\n",
    "        .drop_duplicates(subset=\"sequence_naked\", keep=\"first\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return df3\n",
    "\n",
    "\n",
    "def prep_csv(csv_path):\n",
    "    \"\"\"Parse a .csv file using pandas.\n",
    "\n",
    "    add naked sequences (without PTMs) & the sequence length\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if df is None or df.empty:\n",
    "        raise ValueError(f\"{csv_path} is empty\")\n",
    "    df = df.rename(columns={\"preds\": \"sequence\"})\n",
    "    if \"sequence\" not in df.columns:\n",
    "        raise KeyError(f\"'sequence' column is missing in the file: {csv_path}\")\n",
    "    df.reset_index(drop=True)\n",
    "    df1 = df.assign(sequence_naked=df[\"sequence\"].str.replace(pattern, \"\", regex=True))\n",
    "    df2 = df1.assign(nAA=df1[\"sequence_naked\"].str.len())\n",
    "    df3 = (\n",
    "        df2.sort_values(by=\"sequence_naked\")\n",
    "        .drop_duplicates(subset=\"sequence_naked\", keep=\"first\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return df3\n",
    "\n",
    "\n",
    "def fasta_to_df(fasta_file):\n",
    "    \"\"\"Parse a .fasta file using biopython.\n",
    "\n",
    "    add UniProt ID e.g. P02754\n",
    "    input=path to .fasta file\n",
    "    output=pandas dataframe\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        protein_id = record.id\n",
    "        description = record.description\n",
    "        sequence = str(record.seq)\n",
    "        if not sequence:\n",
    "            raise ValueError(\n",
    "                f\"Record with ID '{protein_id}' has no sequence in the fasta file.\"\n",
    "            )\n",
    "\n",
    "        data.append((protein_id, description, sequence))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"Protein_ID\", \"Description\", \"Sequence\"])\n",
    "    df1 = df.assign(UniProt_ID=df[\"Protein_ID\"].str.split(\"|\").str[1])\n",
    "\n",
    "    return df1\n",
    "\n",
    "\n",
    "def filter_casanovo(df):\n",
    "    \"\"\"Filter a de novo output file based on the maximum value below 0.\n",
    "\n",
    "    search_engine_score[1] is a score assigned to each prediction by Casanovo, max=1,\n",
    "    if negative then outside the mass tolerance\n",
    "    \"\"\"\n",
    "    np_array = df[\"search_engine_score[1]\"].to_numpy()\n",
    "    max_below_zero = np_array[np_array < 0].max()\n",
    "    df1 = df[df[\"search_engine_score[1]\"] >= max_below_zero]\n",
    "    return df1\n",
    "\n",
    "\n",
    "def filter_instanovo(df):\n",
    "    \"\"\"TODO!\"\"\"\n",
    "    return df\n",
    "\n",
    "\n",
    "# prepare overlapping sequence tags for string matching\n",
    "def get_seq_tags(sequence, k):\n",
    "    \"\"\"Generate overlapping sequence tags of size k.\"\"\"\n",
    "    return set(sequence[i : i + k] for i in range(len(sequence) - k + 1))\n",
    "\n",
    "\n",
    "def matching_count(df, df1, k, chunk_size=10000):\n",
    "    \"\"\"Match de novo-based tags with database tags.\n",
    "\n",
    "    I=L in a reference database\n",
    "    inputs=path to fasta, filtered casanovo output dataframe, tag size=k, chunk size=10000 for processing\n",
    "    output=pandas dataframe\n",
    "    \"\"\"\n",
    "    sequence_set = get_seq_tags(\n",
    "        \"\".join(chain.from_iterable(df1[\"sequence_naked\"].astype(str))), k\n",
    "    )\n",
    "    print(f\"📝 {len(sequence_set)} tags regenerated. Starting matching...\")\n",
    "    result_df = pd.DataFrame()\n",
    "    for start in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[start : start + chunk_size].copy()\n",
    "        chunk[\"seq_tags\"] = (\n",
    "            chunk[\"Sequence\"]\n",
    "            .astype(str)\n",
    "            .str.replace(\"I\", \"L\")\n",
    "            .apply(lambda x: get_seq_tags(x, k))\n",
    "        )\n",
    "        chunk[\"matched_count\"] = chunk[\"seq_tags\"].apply(\n",
    "            lambda seq_tags: len(seq_tags & sequence_set)\n",
    "        )\n",
    "        chunk = chunk.assign(\n",
    "            matched=chunk[\"matched_count\"].apply(lambda x: 1 if x >= 2 else 0)\n",
    "        )\n",
    "        result_df = pd.concat([result_df, chunk], ignore_index=True)\n",
    "    total_matches = result_df[\"matched_count\"].sum()\n",
    "    print(f\"Completed! {total_matches} matched 👍 \")\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# get tryptic peptides per database entry\n",
    "def count_tryptic_peptides(sequence):\n",
    "    \"\"\"Count tryptic peptides in a protein sequence.\"\"\"\n",
    "    pattern = r\"(?<=[KR])\"\n",
    "\n",
    "    peptides = re.split(pattern, sequence)\n",
    "\n",
    "    filtered_peptides = [peptide for peptide in peptides if len(peptide) >= 6]\n",
    "\n",
    "    return len(filtered_peptides)\n",
    "\n",
    "\n",
    "# prepare a dataframe for NB classification\n",
    "def prep_Bayes(df):\n",
    "    \"\"\"Prepare a dataframe for Naive Bayes classification.\"\"\"\n",
    "    print(\"🧑‍💻 Start Bayes probabilistic ranking...\")\n",
    "    df1 = df.assign(\n",
    "        length=df[\"Sequence\"].astype(str).str.len(),\n",
    "        tryptic_count=df[\"Sequence\"].apply(count_tryptic_peptides),\n",
    "        tag_count=df[\"seq_tags\"].apply(len),\n",
    "    )\n",
    "    df2 = df1.assign(\n",
    "        SAF=df1[\"matched_count\"] / df1[\"length\"],\n",
    "        try_ratio=df1[\"tryptic_count\"] / df1[\"tag_count\"],\n",
    "    )\n",
    "    return df2\n",
    "\n",
    "\n",
    "def get_bayes_ranking_test(df, threshold=0.95):\n",
    "    \"\"\"Ranking matched proteins based on SAF and try_ratio.\n",
    "\n",
    "    values normalised before NB classification\n",
    "    most probable matches (>= 95%) are shortlised\n",
    "    \"\"\"\n",
    "    m = prep_Bayes(df)\n",
    "    required_columns = {\"SAF\", \"try_ratio\", \"matched\"}\n",
    "    if not required_columns.issubset(m.columns):\n",
    "        missing_cols = required_columns - set(m.columns)\n",
    "        raise ValueError(f\"Missing columns in DataFrame: {missing_cols}\")\n",
    "    m1 = m[m[\"tag_count\"] > 0]\n",
    "    X = m1[[\"SAF\", \"try_ratio\"]].to_numpy()\n",
    "    y = m1[\"matched\"].to_numpy()\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X.reshape(-1, 1)).reshape(*X.shape)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=7\n",
    "    )\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    y_pred = gnb.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(\n",
    "        f\"✅ Gaussian Naive Bayes model ▶️ accuracy:{accuracy:.4f}, precision:{precision:.4f}, f1:{f1:.4f}\"\n",
    "    )\n",
    "    # whole_pred = gnb.predict(X_scaled)\n",
    "    class_probabilities = gnb.predict_proba(X_scaled)\n",
    "    m2 = m1.assign(pred=class_probabilities[:, 1])\n",
    "    m3 = m2[m2[\"pred\"] >= threshold]\n",
    "    return m3\n",
    "\n",
    "\n",
    "# combine previous functions together to output a shortlisted .fasta\n",
    "\n",
    "\n",
    "def matching_ranking_to_fasta_mztab(mztab_path, fasta_df):\n",
    "    \"\"\"Generate a fasta file based on the matched proteins.\"\"\"\n",
    "    denovo_df = prep_mztab(mztab_path)\n",
    "    denovo_df = filter_casanovo(denovo_df)\n",
    "    filestem = os.path.splitext(mztab_path)[0]\n",
    "    return matching_ranking_to_fasta(denovo_df, fasta_df, filestem)\n",
    "\n",
    "\n",
    "def matching_ranking_to_fasta_csv(csv_path, fasta_df):\n",
    "    \"\"\"Generate a fasta file based on the matched proteins.\"\"\"\n",
    "    denovo_df = prep_csv(csv_path)\n",
    "    denovo_df = filter_instanovo(denovo_df)\n",
    "    filestem = os.path.splitext(csv_path)[0]\n",
    "    return matching_ranking_to_fasta(denovo_df, fasta_df, filestem)\n",
    "\n",
    "\n",
    "def matching_ranking_to_fasta(denovo_df, fasta_df, filestem):\n",
    "    \"\"\"Generate a fasta file based on the matched proteins.\"\"\"\n",
    "    k = int(denovo_df[\"nAA\"].median())\n",
    "    m = matching_count(fasta_df, denovo_df, k, chunk_size=10000)\n",
    "    m1 = get_bayes_ranking_test(m)\n",
    "    seq_records = []\n",
    "    for _index, row in m1.iterrows():\n",
    "        header_id = f\"{row['Description']}\"\n",
    "        sequence = Seq(row[\"Sequence\"])\n",
    "        description = \"\"\n",
    "        seq_record = SeqRecord(sequence, id=header_id, description=description)\n",
    "        seq_records.append(seq_record)\n",
    "\n",
    "    output_fasta_filepath = f\"{filestem}_matched.fasta\"\n",
    "\n",
    "    with open(output_fasta_filepath, \"w\") as output_file:\n",
    "        SeqIO.write(seq_records, output_file, \"fasta\")\n",
    "    print(f\"🎊 Number of protein entries in the output fasta: {m1.shape[0]}\")\n",
    "\n",
    "\n",
    "# generate a de novo-first, experiment-specific .fasta for each input\n",
    "def process_all_mztab_files(folder_path, database_path):\n",
    "    \"\"\"Process all .mztab files in a folder.\"\"\"\n",
    "    mztab_filepaths = glob.glob(f\"{folder_path}/*.mztab\")\n",
    "    print(f\"🗂️ {len(mztab_filepaths)} file(s) collecting from {folder_path}...\")\n",
    "    fas = fasta_to_df(database_path)\n",
    "    fasta_df = pd.DataFrame.from_dict(fas)\n",
    "    print(f\"⬆️ {database_path} loaded\")\n",
    "    print(f\"📤 No. of proteins in the reference fasta: {fasta_df.shape[0]}\")\n",
    "\n",
    "    for mztab_filepath in mztab_filepaths:\n",
    "        print(f\"🚀 Processing file: {mztab_filepath}\")\n",
    "        matching_ranking_to_fasta_mztab(mztab_filepath, fasta_df)\n",
    "\n",
    "\n",
    "def process_all_csv_files(folder_path, database_path):\n",
    "    \"\"\"Process all .csv files in a folder.\"\"\"\n",
    "    csv_filepaths = glob.glob(f\"{folder_path}/*.csv\")\n",
    "    print(f\"🗂️ {len(csv_filepaths)} file(s) collecting from {folder_path}...\")\n",
    "    fasta_df = fasta_to_df(database_path)\n",
    "    print(f\"⬆️ {database_path} loaded\")\n",
    "    print(f\"📤 No. of proteins in the reference fasta: {fasta_df.shape[0]}\")\n",
    "\n",
    "    for csv_filepath in csv_filepaths:\n",
    "        print(f\"🚀 Processing file: {csv_filepath}\")\n",
    "        matching_ranking_to_fasta_csv(csv_filepath, fasta_df)\n",
    "\n",
    "\n",
    "def process_all_files(folder_path, database_path, algorithm):\n",
    "    \"\"\"Process all files in a folder.\"\"\"\n",
    "    if algorithm == \"instanovo\":\n",
    "        process_all_csv_files(folder_path, database_path)\n",
    "    elif algorithm == \"casanovo\":\n",
    "        process_all_mztab_files(folder_path, database_path)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid algorithm name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run _de novo_ peptide sequencing algorithm\n",
    "\n",
    "folder = glob.glob(f\"{folder_path}/*.{file_type}\")\n",
    "\n",
    "\n",
    "if algorithm == \"instanovo\":\n",
    "    print(\"🔍 Running InstaNovo...\")\n",
    "    for instrument_file in folder:\n",
    "        print(f\"🚀 Processing file: {instrument_file}\")\n",
    "        base, ext = instrument_file.rsplit(\".\", 1)\n",
    "        output_path = f\"{base}_{algorithm}.csv\"\n",
    "        if use_default:\n",
    "            if not os.path.isfile(\"instanovo_extended.ckpt\"):\n",
    "                print(\"⏬ Downloading InstaNovo checkpoint\")\n",
    "                run_command(\n",
    "                    \"curl -LRO https://github.com/instadeepai/InstaNovo/releases/download/1.0.0/instanovo_extended.ckpt\"\n",
    "                )\n",
    "            if not os.path.isfile(output_path):\n",
    "                run_command(\n",
    "                    f\"python -m instanovo.transformer.predict data_path={instrument_file} model_path='instanovo_extended.ckpt' denovo=True output_path={output_path}\"\n",
    "                )\n",
    "        else:\n",
    "            # TODO add config\n",
    "            if not os.path.isfile(output_path):\n",
    "                run_command(\n",
    "                    f\"python -m instanovo.transformer.predict data_path={instrument_file} model_path={checkpoint} denovo=True output_path={output_path}\"\n",
    "                )\n",
    "elif algorithm == \"casanovo\":\n",
    "    print(\"🔍 Running Casanovo...\")\n",
    "    for instrument_file in folder:\n",
    "        print(f\"🚀 Processing file: {instrument_file}\")\n",
    "        base, ext = instrument_file.rsplit(\".\", 1)\n",
    "        output_path = f\"{base}_{algorithm}.mztab\"\n",
    "        if use_default:\n",
    "            run_command(f\"casanovo sequence {instrument_file} -v info -o {output_path}\")\n",
    "        else:\n",
    "            run_command(\n",
    "                f\"casanovo sequence {instrument_file} -m {checkpoint} -c {config} -v info -o {output_path}\"\n",
    "            )\n",
    "else:\n",
    "    raise ValueError(\"Invalid algorithm name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CvDZIzfwrxbc",
    "outputId": "5114c954-2520-4fa8-e909-555fb99cf97f"
   },
   "outputs": [],
   "source": [
    "#@title Convert de novo results to .fasta per experiment\n",
    "\n",
    "if use_SwissProt:\n",
    "    url = \"https://ftp.uniprot.org/pub/databases/uniprot/knowledgebase/complete/uniprot_sprot.fasta.gz\"\n",
    "    output_file = \"uniprot_sprot.fasta.gz\"\n",
    "    sprot_path = \"uniprot_sprot.fasta\"\n",
    "    if not os.path.isfile(sprot_path):\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                shutil.copyfileobj(response.raw, f)\n",
    "            print(f\"{output_file} downloaded successfully.\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"Failed to download {output_file}, status code: {response.status_code}\"\n",
    "            )\n",
    "        with gzip.open(output_file, \"rb\") as f_in:\n",
    "            with open(sprot_path, \"wb\") as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "    process_all_files(folder_path, sprot_path, algorithm)\n",
    "\n",
    "else:\n",
    "    process_all_files(folder_path, database_path, algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPviBaPleNLy6EHMo/BtU7R",
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "mount_file_id": "1zYQy-qQw4qQuJJFziotDxoXyl8MHDf0r",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "orthrus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "-1.-1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
